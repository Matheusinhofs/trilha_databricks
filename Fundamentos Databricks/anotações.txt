- Fundamentos de Databricks
    - Cenários complexos de dados por haver silos de dados e informações
    - Muitas ferramentas, diferentes sistemas
    - Então foi criado o Data Warehouse para manter os dados em um local e distribuir em data marts
    - Com dados não estruturados e novos desafios surgiram o Data Lake e o Lakehouse 
    - Com novos desafios de negócio, o lakehouse tem o melhor dos mundos de data lake e data warehouse
    - O Databricks possui features para diferentes ferramentas e soluções, como engenheiros, analistas e cientistas de dados
    - Pelo unity catalog pode trabalhar com a parte de segurança, colaboração, qualidade e gerenciamento

- Unity Catalog
    - Abordagem moderna de governança de dados
    - Visão geral de governança
        - Controle de acesso aos dados 
        - Auditoria de acesso aos dados   
            - Registra quem esta acessando o que 
        -  Linhas de origem e destino dos dados 
            - Rastrear as origens (upstream) e os consumidores (downstream) dos dados
        - Descoberta de dados (Data Discovery)
    
    - Arquitetura do Unity Catalog
        - Metastore: é o armazenamento físico do que utilizamos no databricks, utiliza cloud definida por ele. 
        - Catalog: é onde temos o armazenamento lógico do schema e database
            - Schema: agrupar as tabelas em partes dentro do catálogo
                - Objetos dentro do schema:
                    - Tabelas 
                    - View
                    - Volume
                    - Function 
                    - Model
            - Delta Table: garante que vai ser um arquivo comprimido com as características de uma transação ACID
                - Possui um LOG com as alterações que foram feitas no parquet
                - O arquivo é sempre um parquet mas com o LOG de alteração salvo
            - Tabelas Managed vs Unmanaged 
                - Tabalas Managed são tabelas que são salvas no databricks com os dados e os metadados
                - Já as tabelas unmanaged são tabelas em que apenas o metadado é salvo no databricks 
                - Quando é feito o drop table, as tabelas gerenciadas são excluídos os dados e metadados. E as externas são excluídos apenas os metadados
                - A sintaxe é muito semelhante para criação mas quando é salvo uma tabela externa precisa do LOCATION e onde é salvo esta tabela
                - As tabelas gerenciadas são gerenciadas automaticamente, com a possibilidade de auto tune, otimização preditiva, caching e etc. Já as externas você precisa fazer tudo isso
                - As tabelas gerenciadas aceita apenas Delta, já as externas vários formatos de arquivo como CSV, JSON, PARQUET
            - View: permite criar visualizações de uma tabela mas de forma lógica, semelhante ao SQL  
            - Volume: permite salvar arquivos de diferentes tipos, são raw files 
                - Aceita qualquer tipo de arquivo
                - O uso do volume permite o trabalho de difetentes times que trabalham com diferentes tipos de arquivos
                    - Ex: time de ciência de dados que trabalha com a predição através de imagens 
                - É possível criar uma view temporária que funciona enquanto aquele cluster estiver ligado, algo que é gerado no contexto 
            - Function: é semelhante ao stored procedures, uma forma de gravar KPIs dentro do SQL para reutilizar depois 
            - Model: permite criar os modelos de machine learning e gerenciar ele no unity catalog

        - External Storage Access: permite acessar dados que estão salvos fora do databricks 
            - Tudo que esta externo ao catalog e precisa declarar a URL
        - Query Federation: permite consultar dados fora do databricks sem movê-los para dentro do databricks 
        - Delta sharing: Padrão aberto de compartilhamento de dados seguro, criado pelo databricks
        - A parte de governança do databricks permite direcionar o que cada usuário ou grupo pode fazer no databricks (querys, catalog,...) 
        - Cada vez que a tabela é modificada, ele salva o estado que ela estava. 
            - Isso permite o time travel da tabela, podendo voltar a um estado que você deseja
- Lakeflow 
    - Atende o desafio de ingestão de dados de múltiplas fontes de dados 
        - Múltiplos arquivos de txt, excel,...
        - Sistemas legados
        - Database
        - Saas Application
    - Forma unificada de engenharia de dados, com conectores, declaratives pipelines e os Jobs 
    - Lakeflow Connect:
        - Possui alguns conectores próprios do databricks, vai só passando os parâmetros necessários ou logins no sistema
        - Tem conectores com parceiros homologados pelo databricks 
        - Os conectores padrões permitem que o databricks leia arquivos como txt,csv e etc em diversas clouds 
    - Existe uma hierarquia de criação de Catalogo -> Schemas -> Volumes    
    - Para a aula de lakeflow foi criado o catálogo, schema para a arquitetura medalhão, criação dos volumes e consumo dos dados para gerar as tabelas  
    - Os volumes são containers de arquivos no Unity Catalog 
        - Permite armazenar arquivos não estruturados e semi estruturados
        - Além de garantir governança e controle de acesso granular
    - Ao trocar de notebook sempre verificar qual schema e catálogo estão sendo utilizados
    - Você pode até realizar uma query dos arquivos que foram lançados no volume 
    - Batch com CTA
        - Create table select * from ()
        - Ele faz algo muito semelhante a uma CTE 
    -     
